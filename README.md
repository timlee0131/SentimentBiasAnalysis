# Analysis on Sentiment Bias that exist in popular word embeddings (GloVe, ConceptNet) pertaining to ethnicity and geo-location

This notebook investigates bias among popular word embeddings in the application domain of sentiment analysis. Specifically, we investigate whether or not these word embeddings contain bias toward the names of countries. These country names are grouped based on varying degrees of economic status as designated by the World Bank.

**Word embeddings** are dense vector representations of words or phrases that capture semantic and syntactic relationships between words based on their context in a corpus of text. Word embeddings are used to convert textual data into numerical format to be used in machine learning algorithms. 

Bias in machine learning can have far-reaching and often unforseen consequences. It is of upmost importance that researchers understand the biases that their models are prone to and the biases that models are learning from the training data. Awareness of such biases means potential issues can be handled premptively or limitations on a model's use cases can be put in place. Otherwise there could be downstream impacts that, when undetected, could have detrimental impacts on society and inequality.


Consider the following hypothetical example with regards to our investigation concerning country names. Let us suppose that a bias exists in a machine learning model with a text-embedding component where the names of countries with lower economic rankings have a more negative sentiment than the names of countries from a higher economic category. Let us further suppose that this model is being used in production at a major bank that specializes in providing infrastructure development loans to countries. In our hypothetical scenario, this machine learning model is being used to generate memos that summarize the various loan applications and argue for or against moving forward with them. These generated memos will be provided to different decision makers in the bank who will decide which applications to approve.


If a country has a lower economic standing, then the summary and arguments generated by the model could have a more negative tone. This country's approval chances were most likely already low due to it's empircal numbers alone. Now in addition to this, the machine-learning-generated-memo being used to influence the loan decision could contain a more negative tone. This puts the country in an even less likely position to get approved for the infrastructural development loan. Such a scenario could deny a lower income country the opportunity it needs to help its people and the economic divide between higher and lower income countries could be further deepened.


The implications of such a bias could be further extended to areas such as policy-making or humitarian aid to name a few. If such a bias exists in the text embeddings used to develop machine learning models, researchers should be aware of it so that they can better prevent the models from perpetuating economic inequality on a global stage.